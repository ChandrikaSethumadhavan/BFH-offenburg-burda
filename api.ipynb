{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "504186c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4381a118",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"\"\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "header = {\"Authorization\":API_KEY}\n",
    "data = {\"item\":\"\"}\n",
    "incoming = requests.get(url,headers=header,data=data)\n",
    "if incoming.status_code==200:\n",
    "    print(incoming.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b6131c",
   "metadata": {},
   "source": [
    "Translating text from German to English\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bad51bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muthu\\Documents\\bfh\\BFH-offenburg-burda\\bfh\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import MarianMTModel, MarianTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9f4a836",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muthu\\Documents\\bfh\\BFH-offenburg-burda\\bfh\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\muthu\\.cache\\huggingface\\hub\\models--Helsinki-NLP--opus-mt-de-en. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\muthu\\Documents\\bfh\\BFH-offenburg-burda\\bfh\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "model_name = 'Helsinki-NLP/opus-mt-de-en'\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "model = MarianMTModel.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6762033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text):\n",
    "    # Tokenize the input text\n",
    "    tokenized_text = tokenizer.prepare_seq2seq_batch([text], return_tensors='pt')\n",
    "    \n",
    "    # Perform the translation\n",
    "    translation = model.generate(**tokenized_text)\n",
    "    \n",
    "    # Decode the translated text\n",
    "    translated_text = tokenizer.decode(translation[0], skip_special_tokens=True)\n",
    "    \n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b2ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Egg\n"
     ]
    }
   ],
   "source": [
    "src_texts = [\"ei\"]\n",
    "inputs = tokenizer(src_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "translated = model.generate(**inputs)\n",
    "\n",
    "# Decode and print result\n",
    "translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "print(translated_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "979d3d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Egg\n",
      "Oil\n",
      "Flour\n"
     ]
    }
   ],
   "source": [
    "inputs = [\"Ei\",\"öl\",\"mehl\"]\n",
    "for src_texts in inputs:\n",
    "    inputs = tokenizer(src_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated = model.generate(**inputs)\n",
    "\n",
    "    # Decode and print result\n",
    "    translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    print(translated_texts[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29973348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single recipe: ['Marzipan', 'Weizenmehl (Type 405)', 'Trockenhefe', 'Zucker', 'Vanillezucker', 'gemahlenen Mandeln', 'Stollengewürz', 'pflanzliche Milch', 'vegane Butter (Zimmertemperatur)', 'Rosinen', 'Orangeat', 'Zitronat', 'Saft (oder Rum)', 'Etwas Mehl zum Arbeiten', 'Puderzucker', 'Vanillinzucker', 'gemahlene Mandeln', 'veganeButter (Zimmertemperatur)', 'EtwasMehl zum Arbeiten']\n",
      "Global: ['Marzipan', 'Weizenmehl (Type 405)', 'Trockenhefe', 'Zucker', 'Vanillezucker', 'gemahlenen Mandeln', 'Stollengewürz', 'pflanzliche Milch', 'vegane Butter (Zimmertemperatur)', 'Rosinen', 'Orangeat', 'Zitronat', 'Saft (oder Rum)', 'Etwas Mehl zum Arbeiten', 'Puderzucker', 'Vanillinzucker', 'gemahlene Mandeln', 'veganeButter (Zimmertemperatur)', 'EtwasMehl zum Arbeiten', 'Ei (Größe M)'] ... (743 total)\n",
      "Recipes found: ['Veganes Stollenkonfekt', 'Wickeltorte', 'Orangen-Dessert mit Joghurt']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Iterable\n",
    "\n",
    "# --- basic helpers ---\n",
    "def clean_spaces(s: str) -> str:\n",
    "    \"\"\"Collapse multiple spaces and trim.\"\"\"\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def join_ingredient_parts(item: dict) -> str:\n",
    "    \"\"\"Build a readable ingredient string including prefix/suffix if available.\"\"\"\n",
    "    prefix = item.get(\"prefix\") or \"\"\n",
    "    name = (item.get(\"ingredient\") or {}).get(\"name\", \"\") or \"\"\n",
    "    suffix = item.get(\"suffix\") or \"\"\n",
    "    text = clean_spaces(f\"{prefix}{name}\")\n",
    "    if suffix:\n",
    "        text = clean_spaces(f\"{text} {suffix}\")\n",
    "    return text\n",
    "\n",
    "def iter_ingredients(recipe: dict) -> Iterable[dict]:\n",
    "    \"\"\"Iterate through all ingredient items in a recipe.\"\"\"\n",
    "    for block in recipe.get(\"ingredientBlocks\", []):\n",
    "        for it in block.get(\"ingredients\", []):\n",
    "            yield it\n",
    "    # some recipes also have ingredients listed inside content sections\n",
    "    for section in recipe.get(\"content\", []):\n",
    "        for it in section.get(\"ingredients\", []) or []:\n",
    "            yield it\n",
    "\n",
    "def dedup_keep_order(items: List[str]) -> List[str]:\n",
    "    \"\"\"Remove duplicates but keep the original order.\"\"\"\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for x in items:\n",
    "        key = x.lower()\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            result.append(x)\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- load the JSON file ---\n",
    "with open(\"einfachbacken_export_200_recipes_1.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "recipes: List[Dict[str, Any]] = data.get(\"data\", {}).get(\"recipeExport\", [])\n",
    "\n",
    "\n",
    "# --- single recipe ---\n",
    "def ingredients_for_recipe(recipes: List[dict], recipe_name: str, dedup: bool = True) -> List[str]:\n",
    "    recipe = next((r for r in recipes if r.get(\"name\") == recipe_name), None)\n",
    "    if not recipe:\n",
    "        return []\n",
    "    items = [join_ingredient_parts(it) for it in iter_ingredients(recipe)]\n",
    "    return dedup_keep_order(items) if dedup else items\n",
    "# --- all recipes combined ---\n",
    "def ingredients_global(recipes: List[dict], dedup: bool = True) -> List[str]:\n",
    "    items = []\n",
    "    for r in recipes:\n",
    "        items.extend(join_ingredient_parts(it) for it in iter_ingredients(r))\n",
    "    return dedup_keep_order(items) if dedup else items\n",
    "\n",
    "\n",
    "# --- per-recipe dictionary ---\n",
    "def ingredients_per_recipe(recipes: List[dict], dedup: bool = True) -> Dict[str, List[str]]:\n",
    "    result = {}\n",
    "    for r in recipes:\n",
    "        items = [join_ingredient_parts(it) for it in iter_ingredients(r)]\n",
    "        result[r.get(\"name\", \"Unnamed\")] = dedup_keep_order(items) if dedup else items\n",
    "    return result\n",
    "\n",
    "\n",
    "# --- example usage ---\n",
    "one_recipe = ingredients_for_recipe(recipes, \"Veganes Stollenkonfekt\")\n",
    "print(\"Single recipe:\", one_recipe)\n",
    "\n",
    "all_ingredients = ingredients_global(recipes)\n",
    "print(\"Global:\", all_ingredients[:20], f\"... ({len(all_ingredients)} total)\")\n",
    "\n",
    "by_recipe = ingredients_per_recipe(recipes)\n",
    "print(\"Recipes found:\", list(by_recipe.keys())[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90d83b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_texts_all = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e707e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "for src_texts in all_ingredients:\n",
    "    inputs = tokenizer(src_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    translated = model.generate(**inputs)\n",
    "\n",
    "    # Decode and print result\n",
    "    translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "    # print(translated_texts[0])\n",
    "    translated_texts_all.append(translated_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae35dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cf381c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\".csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bfh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
